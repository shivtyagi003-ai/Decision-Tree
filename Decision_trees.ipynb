{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xnqrs4pYRiUj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Information Gain, and how is it used in Decision Trees?\n",
        "- Information Gain (IG) is a measure used in Decision Trees to decide which feature to split on at each node. It tells us how much uncertainty (impurity) in the data is reduced after splitting on a particular attribute.\n",
        "How Information Gain Is Used in Decision Trees?\n",
        "1- Start with the full dataset at the root.\n",
        "2- Calculate entropy of the dataset.\n",
        "3- For each feature:\n",
        "->Split the data based on that feature.\n",
        "->Compute the weighted entropy after the split.\n",
        "->Calculate Information Gain.\n",
        "4- Choose the feature with the highest Information Gain.\n",
        "5- Repeat the process recursively for child nodes until stopping criteria are met.\n",
        "\n",
        "2. What is the difference between Gini Impurity and Entropy?\n",
        "Hint: Directly compares the two main impurity measures, highlighting strengths,\n",
        "weaknesses, and appropriate use cases.\n",
        "- Both Gini Impurity and Entropy are measures of node impurity used in Decision Tree classifiers to decide the best split. They quantify how mixed the classes are in a dataset, but they differ in formulation, interpretation, and usage.\n",
        "Strength of Gini Impurity:-\n",
        "-> Faster to compute (no logarithms)\n",
        "-> Works well for large datasets\n",
        "Weakness of Gini Impurity:-\n",
        "-> Slightly less precise in evaluating split quality\n",
        "\n",
        "Strength of Entrophy:-\n",
        "-> More theoretically grounded (information theory)\n",
        "-> Better when fine-grained splits matter\n",
        "Weakness of Entrophy:-\n",
        "-> Slower due to logarithmic calculations\n",
        "\n",
        "3. What is Pre-Pruning in Decision Trees?\n",
        "- Pre-pruning (also called early stopping) is a technique used in Decision Trees to stop the tree from growing too deep during training, in order to prevent overfitting.\n",
        "\n",
        "4. What is a Support Vector Machine (SVM)?\n",
        "- A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. Its main goal is to find the best decision boundary (hyperplane) that separates data points of different classes with the maximum margin.\n",
        "\n",
        "5. What is the Kernel Trick in SVM?\n",
        "- The Kernel Trick is a technique used in Support Vector Machines (SVMs) that allows them to handle non-linearly separable data by implicitly mapping the input data into a higher-dimensional feature space, where a linear separation becomes possible.\n",
        "\n",
        "6. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "- The Naïve Bayes classifier is a supervised, probabilistic machine learning algorithm based on Bayes’ Theorem. It is widely used for classification tasks, especially in text classification and spam filtering, because it is simple, fast, and effective.\n",
        "Why is it called \"Naive\"?\n",
        "It is called “Naïve” because it makes a strong simplifying assumption\n",
        "\n",
        "7. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes?\n",
        "- Gaussian naive bayes:- Gaussian Naïve Bayes is a probabilistic classification algorithm based on Bayes’ Theorem, used when the input features are continuous and assumed to follow a Gaussian (normal) distribution within each class.\n",
        "\n",
        "- Multinomial naive bayes:- Multinomial Naïve Bayes is a probabilistic classification algorithm based on Bayes’ Theorem, mainly used when the features represent counts or frequencies of events.\n",
        "\n",
        "It is especially popular in text classification problems such as spam detection and document categorization.\n",
        "\n",
        "- Bernoulli naive bayes:- Bernoulli Naïve Bayes is a probabilistic classification algorithm based on Bayes’ Theorem, used when the features are binary (0 or 1)—that is, they indicate the presence or absence of a feature.\n",
        "\n",
        "It is commonly used in text classification when we care about whether a word appears or not, not how many times it appears."
      ],
      "metadata": {
        "id": "HusU08POR0j-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "gnb = GaussianNB()\n",
        "\n",
        "\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy of Gaussian Naive Bayes:\", accuracy)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxOxuRN1VBu9",
        "outputId": "ac0e5eb2-1840-4931-f40c-f128ce8646d7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naive Bayes: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Decision Tree Classifier using GiniImpurity as the criterion and print the feature importances (practical)\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "importances = model.feature_importances_\n",
        "\n",
        "\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": importances\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(feature_importance_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNtVtdqjW_5j",
        "outputId": "cd882e6c-566f-4c17-e6fb-5f510fcd6360"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    Feature  Importance\n",
            "7       mean concave points    0.691420\n",
            "27     worst concave points    0.065651\n",
            "1              mean texture    0.058478\n",
            "20             worst radius    0.052299\n",
            "22          worst perimeter    0.051494\n",
            "19  fractal dimension error    0.018554\n",
            "21            worst texture    0.017445\n",
            "17     concave points error    0.015931\n",
            "13               area error    0.011983\n",
            "24         worst smoothness    0.009233\n",
            "16          concavity error    0.006276\n",
            "14         smoothness error    0.001237\n",
            "2            mean perimeter    0.000000\n",
            "3                 mean area    0.000000\n",
            "12          perimeter error    0.000000\n",
            "11            texture error    0.000000\n",
            "10             radius error    0.000000\n",
            "9    mean fractal dimension    0.000000\n",
            "6            mean concavity    0.000000\n",
            "8             mean symmetry    0.000000\n",
            "4           mean smoothness    0.000000\n",
            "5          mean compactness    0.000000\n",
            "0               mean radius    0.000000\n",
            "15        compactness error    0.000000\n",
            "18           symmetry error    0.000000\n",
            "23               worst area    0.000000\n",
            "25        worst compactness    0.000000\n",
            "26          worst concavity    0.000000\n",
            "28           worst symmetry    0.000000\n",
            "29  worst fractal dimension    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "\n",
        "print(\"Accuracy with Linear Kernel:\", accuracy_linear)\n",
        "print(\"Accuracy with RBF Kernel:\", accuracy_rbf)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLmskhylXJ8N",
        "outputId": "5c4e8831-dedc-4668-97f4-e75d803b0f12"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Linear Kernel: 0.9722222222222222\n",
            "Accuracy with RBF Kernel: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CVeG5oKtYXBU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}